{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff95ac7f-eb96-43e8-bd3d-544d98638932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#jakas zmiana\n",
    "import os\n",
    "with open('tokens_and_api.txt', 'r') as file:\n",
    "    exec(file.read())\n",
    "\n",
    "#sas_token = tokens in tokens.txt\n",
    "storage_account_name = \"newadbprojektkakastorage\"\n",
    "container_name = \"data\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "print(f\"Skonfigurowano dostęp do: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b73464-a8d5-4001-9335-945b7a5497d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_model = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{base_path}/final_results/df_weather_jfk.csv\")\n",
    "\n",
    "display(df_model.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaebbd0f-6c06-457d-892f-766433a7bdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#usuwamy kolumny czasowe/identyfikatory, zostawiamy parametry lotu i pogody\n",
    "feature_cols = [\"DISTANCE\", \"HOURLYVISIBILITY\", \"HOURLYWindSpeed\", \"DAY_OF_WEEK\"]\n",
    "\n",
    "#linie lotnicze (tekst na liczby)\n",
    "airline_indexer = StringIndexer(inputCol=\"AIRLINE\", outputCol=\"AIRLINE_INDEX\", handleInvalid=\"skip\")\n",
    "\n",
    "#składanie cech w jeden wektor\n",
    "assembler = VectorAssembler(inputCols=feature_cols + [\"AIRLINE_INDEX\"], outputCol=\"raw_features\", handleInvalid=\"skip\")\n",
    "\n",
    "#skalowanie \n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee5bf26-20a5-4315-a75c-dbc06700837b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train_data, test_data = df_model.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "rf_class = RandomForestClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "#pipeline dla klasyfikacji\n",
    "pipeline_class = Pipeline(stages=[airline_indexer, assembler, scaler, rf_class])\n",
    "model_cancel = pipeline_class.fit(train_data)\n",
    "\n",
    "#predykcja\n",
    "predictions_cancel = model_cancel.transform(test_data)\n",
    "\n",
    "#AUC - im bliżej 1, tym lepiej\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"CANCELLED\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_auc.evaluate(predictions_cancel)\n",
    "\n",
    "print(f\"Skuteczność modelu (AUC) dla odwołań: {auc}\")\n",
    "display(predictions_cancel.select(\"CANCELLED\", \"prediction\", \"probability\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188df5e8-80ce-4f2f-9033-8cd1499b5573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions_cancel.filter((col(\"prediction\") == 1) | (col(\"CANCELLED\") == 1))\n",
    "        .select(\"CANCELLED\", \"prediction\", \"probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11e4129-bd0c-4d6c-87c1-9e516de0637f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_cancel.groupBy(\"CANCELLED\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edfb6c8-4b26-4637-af32-b921fb9fd991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df_reg = df_model.filter(col(\"CANCELLED\") == 0)\n",
    "train_reg, test_reg = df_reg.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "gbt_reg = GBTRegressor(labelCol=\"DEPARTURE_DELAY\", featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "#pipeline dla regresji\n",
    "pipeline_reg = Pipeline(stages=[airline_indexer, assembler, scaler, gbt_reg])\n",
    "model_delay = pipeline_reg.fit(train_reg)\n",
    "\n",
    "#predykcja\n",
    "predictions_delay = model_delay.transform(test_reg)\n",
    "\n",
    "#RMSE\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"DEPARTURE_DELAY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions_delay)\n",
    "\n",
    "print(f\"Średni błąd przewidywania (RMSE): {rmse} minut\")\n",
    "display(predictions_delay.select(\"DEPARTURE_DELAY\", \"prediction\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e242dcf7-d64c-4add-aace-f70170589a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section with df_weather_jfk_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546714bb-9942-437b-8c02-79f2616b0e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_model_2 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{base_path}/final_results/df_weather_jfk.csv\")\n",
    "\n",
    "display(df_model_2.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277a6bd9-04e3-4c31-b822-89ca2fe9957b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pyspark.sql.functions import to_timestamp, hour, month\n",
    "\n",
    "# Filter and drop columns using Spark DataFrame syntax\n",
    "df = df_model_2.filter(df_model_2['DEPARTURE_DELAY'] >= 0)\n",
    "cols_to_drop = ['FLIGHT_NUMBER', 'CANCELLED', 'WEATHER_DELAY', 'DeparturedTimestamp','ORIGIN_AIRPORT','FLIGHT_NUMBER','AIRLINE','DISTANCE','DAY_OF_WEEK']\n",
    "df = df.drop(*cols_to_drop)\n",
    "\n",
    "# Convert to timestamp and extract hour and month\n",
    "df = df.withColumn('ScheduledTimestamp', to_timestamp('ScheduledTimestamp'))\n",
    "df = df.drop('ScheduledTimestamp')\n",
    "\n",
    "# Convert to pandas DataFrame for sklearn\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# One-hot encoding\n",
    "df_pd = pd.get_dummies(\n",
    "    df_pd,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr = df_pd.corr()\n",
    "sns.heatmap(corr[['DEPARTURE_DELAY']].sort_values(by='DEPARTURE_DELAY', ascending=False), annot=True, cmap='coolwarm')\n",
    "plt.title('Korelacja cech z opóźnieniem (DEPARTURE_DELAY)')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "X = df_pd.drop('DEPARTURE_DELAY', axis=1)\n",
    "y = df_pd['DEPARTURE_DELAY']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importances.nlargest(10).plot(kind='barh')\n",
    "plt.title('Ważność cech w modelu (Feature Importance)')\n",
    "plt.xlabel('Ważność')\n",
    "plt.show() \n",
    "\n",
    "print(\"Dane po czyszczeniu (podgląd):\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nStatystyki modelu:\")\n",
    "print(f\"MSE: {mean_squared_error(y, y_pred):.2f}\")\n",
    "print(f\"R2: {r2_score(y, y_pred):.2f}\")\n",
    "\n",
    "print(\"\\nPrzewidywania vs Rzeczywistość:\")\n",
    "comparison = pd.DataFrame({'Actual': y, 'Predicted': y_pred})\n",
    "\n",
    "try:\n",
    "    display(comparison.head(10))\n",
    "except NameError:\n",
    "    print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78929ba1-7db7-4278-b79a-dde1f949f649",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767802614899}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0638d00f-739b-4355-8ba8-b433352f8610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "df_pandas = df_model_2.toPandas()\n",
    "\n",
    "# Zostawienie tylko delay >= 0\n",
    "df_cleaned = df_pandas[df_pandas['DEPARTURE_DELAY'] >= 0].copy()\n",
    "\n",
    "# Usuwanie zbędnych kolumn\n",
    "columns_to_drop = ['FLIGHT_NUMBER', 'CANCELLED', 'WEATHER_DELAY', 'DeparturedTimestamp']\n",
    "\n",
    "df_cleaned = df_cleaned.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "df_cleaned['ScheduledTimestamp'] = pd.to_datetime(df_cleaned['ScheduledTimestamp'])\n",
    "df_cleaned['HOUR'] = df_cleaned['ScheduledTimestamp'].dt.hour\n",
    "df_cleaned['MONTH'] = df_cleaned['ScheduledTimestamp'].dt.month\n",
    "df_cleaned = df_cleaned.drop(columns=['ScheduledTimestamp'])\n",
    "\n",
    "# Kodowanie zmiennych kategorycznych (One-Hot Encoding)\n",
    "df_model = pd.get_dummies(df_cleaned, columns=['AIRLINE', 'ORIGIN_AIRPORT'], drop_first=True)\n",
    "\n",
    "# ==========================================\n",
    "# Wizualizacja 1: Korelacja\n",
    "# ==========================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr = df_model.corr()\n",
    "sns.heatmap(corr[['DEPARTURE_DELAY']].sort_values(by='DEPARTURE_DELAY', ascending=False), annot=True, cmap='coolwarm')\n",
    "plt.title('Korelacja cech z opóźnieniem (DEPARTURE_DELAY)')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Trenowanie modelu\n",
    "X = df_model.drop('DEPARTURE_DELAY', axis=1)\n",
    "y = df_model['DEPARTURE_DELAY']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Ewaluacja i Ważność cech\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "# ==========================================\n",
    "# Wizualizacja 2: Feature Importance\n",
    "# ==========================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importances.nlargest(10).plot(kind='barh')\n",
    "plt.title('Ważność cech w modelu (Feature Importance)')\n",
    "plt.xlabel('Ważność')\n",
    "plt.show() \n",
    "print(\"Dane po czyszczeniu (podgląd):\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "print(\"\\nStatystyki modelu:\")\n",
    "print(f\"MSE: {mean_squared_error(y, y_pred):.2f}\")\n",
    "print(f\"R2: {r2_score(y, y_pred):.2f}\")\n",
    "\n",
    "print(\"\\nPrzewidywania vs Rzeczywistość:\")\n",
    "comparison = pd.DataFrame({'Actual': y, 'Predicted': y_pred})\n",
    "\n",
    "try:\n",
    "    display(comparison.head(10))\n",
    "except NameError:\n",
    "    print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8cc0e8-a01e-4c8f-8f4e-77d36d4ce82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prediction_models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
